{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81eac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UNCOMMENT THE FOLLOWING LINES IN ORDER TO INSTALL THE DEPENDENCIES\"\"\"\n",
    "#!pip install numpy\n",
    "# !pip install plotly\n",
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "\n",
    "\"\"\"\n",
    "Import pandas to handle and analyse data\n",
    "Import numpy to perform complex calculations on data\n",
    "\"\"\"\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy\n",
    "\n",
    "\"\"\"\n",
    "Import datetime module to process datestamps\n",
    "\"\"\"\n",
    "from datetime import date, timedelta\n",
    "\n",
    "\"\"\"\n",
    "Import os to walk through paths or make directories\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Import plotly library to plot graphs\n",
    "\"\"\"\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pandas.options.plotting.backend = \"plotly\"\n",
    "\n",
    "\"\"\"\n",
    "To evaluate the error and loss\n",
    "\"\"\"\n",
    "from sklearn import metrics\n",
    "\n",
    "\"\"\"\n",
    "Importing Machine Learning Models\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\"\"\"\n",
    "Save Machine Learning Model locally\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "Import warning module to ignore the future warnings\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "Time module to calculate the time required for training\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "import calendar\n",
    "calendar.setfirstweekday(6)\n",
    "\n",
    "print(\"Libraries has been loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597db652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded successfully\n",
      "Columns has been selected successfully\n",
      "Renamed successfully\n",
      "Gathered Unique Entry ID successfully\n",
      "Sorted successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load data from provided csv file\n",
    "\"\"\"\n",
    "raw = pandas.read_csv(r\"./Raw Data.csv\")\n",
    "print(\"Data has been loaded successfully\")\n",
    "\n",
    "\"\"\"\n",
    "Select important features only\n",
    "\"\"\"\n",
    "raw = raw[['date', 'entry_id', 'item_id', 'item_unit_id', 'price_min', 'price_max', 'price_avg', 'period', 'currency']]\n",
    "print(\"Columns has been selected successfully\")\n",
    "\n",
    "\"\"\"\n",
    "Rename the columns here\n",
    "to keep the same variable structure\n",
    "\"\"\"\n",
    "raw = raw.rename(columns={'date':'Date',\n",
    "                          'entry_id': 'Entry ID',\n",
    "                          'item_id': 'Item ID',\n",
    "                          'item_unit_id': \"Item Unit ID\",\n",
    "                          'price_min': 'Price Min',\n",
    "                          'price_max': 'Price Max',\n",
    "                          'price_avg': 'Price Avg',\n",
    "                          'period': 'Period',\n",
    "                          'currency':\"Currency\"\n",
    "                         })\n",
    "print(\"Renamed successfully\")\n",
    "\n",
    "\"\"\"\n",
    "Extract distinct Entry ID of each product\n",
    "\"\"\"\n",
    "entryID = raw['Entry ID'].unique().tolist()\n",
    "print(\"Gathered Unique Entry ID successfully\")\n",
    "\n",
    "\"\"\"\n",
    "Groupy raw data by Entry ID\n",
    "Sort by Date stamp\n",
    "\"\"\"\n",
    "raw = raw.sort_values([\"Entry ID\", \"Date\"],ascending=True)\n",
    "print(\"Sorted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3748099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Union between two dataframes\n",
    "Union based of specific column between dataframes\n",
    "\"\"\"\n",
    "def mergeRight(DF1, DF2, targetColumn):\n",
    "    return pandas.merge(DF1, DF2, on = targetColumn, how = \"right\")\n",
    "\n",
    "\"\"\"\n",
    "Intersection between two dataframes\n",
    "Intersection based of common column between dataframes\n",
    "\"\"\"\n",
    "def mergeLeft(DF1, DF2, targetColumn):\n",
    "    return pandas.merge(DF1, DF2, on = targetColumn, how = \"left\")\n",
    "\n",
    "\"\"\"\n",
    "Generate the range of date stamps and type is dataframeProcess with start and end dates\n",
    "interval: day (d), week(w), month(m)\n",
    "\"\"\"\n",
    "def getDateList(start_date, end_date, frequency):\n",
    "    if frequency == \"d\" or frequency == \"w\" or frequency == \"m\":\n",
    "        return pandas.DataFrame(pandas.date_range(start_date,\n",
    "                                                  end_date,\n",
    "                                                  freq = frequency) + timedelta(days=1)).rename(columns={0:'Date'}).astype(str)\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR IN GENERATING DATES LIST: WRONG FREQUENCY PARAMETER\")\n",
    "\n",
    "\"\"\"\n",
    "Collect missing dates data and add it into raw data\n",
    "It also returns the percentage of how much missing datestamp in the data \n",
    "\"\"\"     \n",
    "def collectMissingData(data, start_date, end_date, frequency):\n",
    "    try:\n",
    "        dateRanges = getDateList(start_date, end_date, frequency)\n",
    "        data[\"Date\"] = data[\"Date\"].astype(str)\n",
    "        LM = mergeLeft(dateRanges, data[\"Date\"].astype(str), \"Date\")\n",
    "        RM = mergeRight(data, LM, \"Date\")\n",
    "        return RM\n",
    "    except Exception as e:\n",
    "        print(\"ERROR IN COLLECTING MISSING DATE INFORMATIONS: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d276fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates the series dataset for forecasting\n",
    "Note: 1D shape data should be converted to list before calling the function for that data or reshape the array/dataframe to (-1,1)\n",
    "Note: Greater than 1D dataframe/array processed normally No need to reshape\n",
    "\"\"\"    \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=False):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pandas.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \"\"\"\n",
    "    input sequence (t-n, ... t-1)\n",
    "    \"\"\"\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \"\"\"\n",
    "    forecast sequence (t, t+1, ... t+n)\n",
    "    \"\"\"\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \"\"\"\n",
    "    put it all together\n",
    "    \"\"\"\n",
    "    agg = pandas.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    \"\"\"\n",
    "    drop rows with NaN values\n",
    "    \"\"\"\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd98e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create directory over path provided\n",
    "Check if the directory exists, otherwise create it\n",
    "\"\"\"\n",
    "def makeDir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\"\"\"\n",
    "Calculate percentage\n",
    "\"\"\"\n",
    "def calculatePercentage(obtain, total):\n",
    "    return (obtain/total)*100\n",
    "\n",
    "\"\"\"\n",
    "Calculate moving average \n",
    "Process only one column of dataframe\n",
    "\"\"\"\n",
    "def movingAverage(data, window):\n",
    "    return data.rolling(window, min_periods=1).mean()\n",
    "\n",
    "\"\"\"\n",
    "Function for applying scaling\n",
    "Min-Max Scaling\n",
    "\"\"\"\n",
    "def minmax_scale(dataframe, Min, Max):\n",
    "    return (dataframe - Min) / (Max - Min)\n",
    "\n",
    "\"\"\"\n",
    "Function for applying reverse scaling\n",
    "Inverse Min-Max Scaling\n",
    "\"\"\"\n",
    "def reverse_minmax_scale(dataframe, Min, Max):\n",
    "    return (dataframe * (Max - Min)) + Min\n",
    "\n",
    "\"\"\"\n",
    "Return the Mean Square Error\n",
    "\"\"\"\n",
    "def meanSquareError(true, predicted):\n",
    "    return metrics.mean_squared_error(true, predicted)\n",
    "\n",
    "\"\"\"\n",
    "Return the Mean Absolute Error\n",
    "\"\"\"\n",
    "def meanAbsouluteError(true, predicted):\n",
    "    return metrics.mean_absolute_error(true, predicted)\n",
    "\n",
    "\"\"\"\n",
    "Return the Mean Absolute Percentage Error\n",
    "\"\"\"\n",
    "def meanAbsolutePercentageError(true, predicted):\n",
    "    return metrics.mean_absolute_percentage_error(true, predicted)\n",
    "\n",
    "\"\"\"\n",
    "Evalute model results and store into \"Results\" List\n",
    "\"\"\"\n",
    "def model_result(Model, trainX, trainY, testX, testY):\n",
    "    Train_mse = meanAbsolutePercentageError(trainY, Model.predict(trainX))\n",
    "    Test_mse = meanAbsolutePercentageError(testY, Model.predict(testX))\n",
    "    return Train_mse, Test_mse\n",
    "\n",
    "\"\"\"\n",
    "Save model weigths to disk\n",
    "\"\"\"\n",
    "def dumpModel(path, model):\n",
    "    pickle.dump(model, open(path, 'wb'))\n",
    "    \n",
    "def getMissingDataPercentage(data):\n",
    "    missing = data[data.columns.tolist()[-1]].isnull().sum()\n",
    "    total = len(data)\n",
    "    percentage = calculatePercentage(missing, total)\n",
    "    return percentage\n",
    "\n",
    "def missingPercentage(data):\n",
    "    initDate = dateInit(data)\n",
    "    data = data[data[\"Date\"] >= initDate]\n",
    "    missing = data[data.columns.tolist()[-1]].isnull().sum()\n",
    "    total = len(data)\n",
    "    percentage = calculatePercentage(missing, total)\n",
    "    return percentage\n",
    "\n",
    "def dateInit(data):\n",
    "    return data[\"Date\"][data.index == data[\"Date\"][~data[data.columns.tolist()[-1]].isnull()].index[0]].iloc[0]\n",
    "\n",
    "def get_week_of_month(year, month, day):\n",
    "    x = pandas.DataFrame(calendar.monthcalendar(year, month))\n",
    "    x = x[x == day].dropna(axis = 0, how = 'all')\n",
    "    return x.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d755af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Linear Regression fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def linearRegression(trainX, trainY, testX, testY):\n",
    "    model = LinearRegression(copy_X= True, fit_intercept= True, positive=False)\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create Decision Tree fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def DTree(trainX, trainY, testX, testY):\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create Random Forest fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def RFTree(trainX, trainY, testX, testY):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create Bagging Regressor fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def BRegressor(trainX, trainY, testX, testY):\n",
    "    model = BaggingRegressor()\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create Extra Trees Regressor fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def EXTree(trainX, trainY, testX, testY):\n",
    "    model = ExtraTreesRegressor()\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create Support Vector Machine Regressor fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def SVMReg(trainX, trainY, testX, testY):\n",
    "    model = svm.SVR()\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create k-nearest neighbors Regressor fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def KNN(trainX, trainY, testX, testY):\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model\n",
    "\n",
    "\"\"\"\n",
    "Create MLP fiiting function, it returns the fitted model along with training and testing loss\n",
    "\"\"\"\n",
    "def NeuralNetwork(trainX, trainY, testX, testY):\n",
    "    model = MLPRegressor(hidden_layer_sizes=(10,5),\n",
    "                         random_state=1,\n",
    "                          activation='relu',\n",
    "                          solver='adam',\n",
    "                          max_iter=2000\n",
    "                         )\n",
    "    model.fit(trainX,trainY)\n",
    "    trainLoss, testLoss = model_result(model, trainX, trainY, testX, testY)\n",
    "    return trainLoss, testLoss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33c34b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYOU CAN COMMENT OR UNCOMMENT THE LINES IN MODEL LIST, IT WILL DECIDE WHETHER TO RUN THAT MODEL FITTING OR NOT\\n\\n*** IMPORTANT ***\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "*** IMPORTANT ***\n",
    "\n",
    "CREATE THE NEW LINE ABOVE AND ADD MORE MACHINE LEARNING MODELS FUNCTION, SIMILAR TO ABOVE PATTERN\n",
    "AFTER CREATING NEW MACHINE LEARNING MODEL FUNCTIONS, ADD ITS NAME AND FUNCTION NAME TO MODEL LIST BELOW\n",
    "\"\"\"\n",
    "modelList = {\n",
    "             #\"Multivariate Linear Regression\": \"linearRegression\",\n",
    "             #\"Decission Tree\": \"DTree\",\n",
    "             #\"Random Forest\": \"RFTree\",\n",
    "             #\"Bagging Regressor\": \"BRegressor\",\n",
    "             #\"Extra Trees Regressor\": \"EXTree\",\n",
    "             \"K-Nearest Neighbors\":\"KNN\",\n",
    "             \"Support Vector Regressor\": \"SVMReg\",\n",
    "             \"Neural Network\": \"NeuralNetwork\"\n",
    "            }\n",
    "\"\"\"\n",
    "YOU CAN COMMENT OR UNCOMMENT THE LINES IN MODEL LIST, IT WILL DECIDE WHETHER TO RUN THAT MODEL FITTING OR NOT\n",
    "\n",
    "*** IMPORTANT ***\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce5090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables to initialize the date stamp generation\n",
    "\"\"\"\n",
    "start_date = date(2019, 1, 1)\n",
    "end_date = date.today()\n",
    "frequency = \"w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6f4a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 41s, sys: 12 s, total: 23min 52s\n",
      "Wall time: 23min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Create dataframe of data ranges between start and end date over frequency/interval of week\n",
    "\"\"\"\n",
    "dataMin = getDateList(start_date, end_date, frequency)\n",
    "dataMax = getDateList(start_date, end_date, frequency)\n",
    "dataAvg = getDateList(start_date, end_date, frequency)\n",
    "\"\"\"\n",
    "Create History dataframe to store important information about each entry ID\n",
    "\"\"\"\n",
    "History = pandas.DataFrame(columns = [\"Entry ID\", \n",
    "                                      \"Item ID\",\n",
    "                                      \"Item Unit ID\",\n",
    "                                      \"Period\",\n",
    "                                      \"Currency\",\n",
    "                                      \"Price Min: Max\",\n",
    "                                      \"Price Max: Max\",\n",
    "                                      \"Price Avg: Max\"\n",
    "                                     ])\n",
    "\n",
    "\"\"\"\n",
    "Extract important information for each entry\n",
    "1. Minimum price\n",
    "2. Maximum price\n",
    "3. Average price\n",
    "3. Collect missing dates and add them to each entry\n",
    "4. Pivot the dataframe to apply further pre-processing cleanly\n",
    "\"\"\"\n",
    "for _ in entryID:\n",
    "    temp = raw[[\"Date\", \"Price Min\"]][raw[\"Entry ID\"] == _]\n",
    "    temp1 = raw[[\"Date\", \"Price Max\"]][raw[\"Entry ID\"] == _]\n",
    "    temp2 = raw[[\"Date\", \"Price Avg\"]][raw[\"Entry ID\"] == _]\n",
    "    temp = collectMissingData(temp, start_date, end_date, frequency)\n",
    "    temp1 = collectMissingData(temp1, start_date, end_date, frequency)\n",
    "    temp2 = collectMissingData(temp2, start_date, end_date, frequency)\n",
    "    temp = temp.rename(columns={\"Price Min\": _ })\n",
    "    temp1 = temp1.rename(columns={\"Price Max\": _ })\n",
    "    temp2 = temp2.rename(columns={\"Price Avg\": _ })\n",
    "    alpha = pandas.DataFrame([_,\n",
    "                              raw[\"Item ID\"][raw[\"Entry ID\"] == _].unique()[0],\n",
    "                              raw[\"Item Unit ID\"][raw[\"Entry ID\"] == _].unique()[0],\n",
    "                              raw[\"Period\"][raw[\"Entry ID\"] == _].unique()[0],\n",
    "                              raw[\"Currency\"][raw[\"Entry ID\"] == _].unique()[0],\n",
    "                              temp[_].max(),\n",
    "                              temp1[_].max(),\n",
    "                              temp2[_].max(),\n",
    "                             ]).T.rename(columns={0:\"Entry ID\",\n",
    "                                                  1:\"Item ID\",\n",
    "                                                  2:\"Item Unit ID\",\n",
    "                                                  3:\"Period\",\n",
    "                                                  4:\"Currency\",\n",
    "                                                  5:\"Price Min: Max\",\n",
    "                                                  6:\"Price Max: Max\",\n",
    "                                                  7:\"Price Avg: Max\"})\n",
    "    History = pandas.concat([History, alpha],axis=0)\n",
    "    dataMin = mergeRight(dataMin, temp, \"Date\")\n",
    "    dataMax = mergeRight(dataMax, temp1, \"Date\")\n",
    "    dataAvg = mergeRight(dataAvg, temp2, \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0fa476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clear the memory: Raw data is no more required for further process\n",
    "\"\"\"\n",
    "raw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef406f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 32s, sys: 7.56 s, total: 48min 40s\n",
      "Wall time: 1h 13min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "History[\"Missing Percentage From Max Count\"] = None\n",
    "History[\"Grade\"] = None\n",
    "for _ in entryID:\n",
    "    percentage = getMissingDataPercentage(dataMin[[\"Date\", _]])\n",
    "    History[\"Missing Percentage From Max Count\"][History[\"Entry ID\"] == _] = percentage\n",
    "History[\"Grade\"][History[\"Missing Percentage From Max Count\"] <= 60 ] = \"B\"\n",
    "History[\"Grade\"][History[\"Missing Percentage From Max Count\"] <= 30 ] = \"A\"\n",
    "History[\"Grade\"][History[\"Missing Percentage From Max Count\"] > 60 ] = \"C\"\n",
    "\n",
    "\"\"\"\n",
    "Add another column in History, the safe max is used in min-max scaling\n",
    "After model training there must be some levarge to process rising values\n",
    "\"\"\"\n",
    "History[\"Price Min: Safe Max\"] = History[\"Price Min: Max\"] * 1.2\n",
    "History[\"Price Max: Safe Max\"] = History[\"Price Max: Max\"] * 1.2\n",
    "\n",
    "\"\"\"\n",
    "Add more columns to History to store train and test dataset paths\n",
    "\"\"\"\n",
    "History[\"Trainable\"] = False\n",
    "\n",
    "History[[\"Price Min: Train X\",\n",
    "         \"Price Min: Train Y\",\n",
    "         \"Price Min: Test X\",\n",
    "         \"Price Min: Test Y\"]] = None\n",
    "\n",
    "History[[\"Price Max: Train X\",\n",
    "         \"Price Max: Train Y\",\n",
    "         \"Price Max: Test X\",\n",
    "         \"Price Max: Test Y\"]] = None\n",
    "\n",
    "\"\"\"\n",
    "Process to create train and test datasets and store them locally while keeping there paths to history dataframe\n",
    "\"\"\"\n",
    "TrainSplit = 0.7\n",
    "\n",
    "PriceMinTrainPathX = \"Dataset/Price Min Dataset/Train X\"\n",
    "PriceMinTrainPathY = \"Dataset/Price Min Dataset/Train Y\"\n",
    "PriceMinTestPathX = \"Dataset/Price Min Dataset/Test X\"\n",
    "PriceMinTestPathY = \"Dataset/Price Min Dataset/Test Y\"\n",
    "\n",
    "PriceMaxTrainPathX = \"Dataset/Price Max Dataset/Train X\"\n",
    "PriceMaxTrainPathY = \"Dataset/Price Max Dataset/Train Y\"\n",
    "PriceMaxTestPathX = \"Dataset/Price Max Dataset/Test X\"\n",
    "PriceMaxTestPathY = \"Dataset/Price Max Dataset/Test Y\"\n",
    "\n",
    "makeDir(PriceMinTrainPathX)\n",
    "makeDir(PriceMinTrainPathY)\n",
    "makeDir(PriceMinTestPathX)\n",
    "makeDir(PriceMinTestPathY)\n",
    "\n",
    "makeDir(PriceMaxTrainPathX)\n",
    "makeDir(PriceMaxTrainPathY)\n",
    "makeDir(PriceMaxTestPathX)\n",
    "makeDir(PriceMaxTestPathY)\n",
    "\n",
    "#Run loop over all entries\n",
    "for _ in entryID:\n",
    "    #Select date column from data\n",
    "    tempMin = pandas.DataFrame(dataMin[\"Date\"])\n",
    "    tempMax = pandas.DataFrame(dataMax[\"Date\"])\n",
    "\n",
    "    tempMin[\"Month\"] = tempMin[\"Date\"].str.split(\"-\").str[1].astype(int)/12\n",
    "    tempMin[\"Month\"] = tempMin[\"Month\"].round(decimals=2)\n",
    "    tempMin[\"Week Number\"] = pandas.DataFrame (get_week_of_month(int(j.split(\"-\")[0]),\n",
    "                            int(j.split(\"-\")[1]),\n",
    "                            int(j.split(\"-\")[2])\n",
    "                            )/5 for j in tempMin[\"Date\"])\n",
    "    \n",
    "    tempMax[\"Month\"] = tempMax[\"Date\"].str.split(\"-\").str[1].astype(int)/12\n",
    "    tempMax[\"Month\"] = tempMax[\"Month\"].round(decimals=2)\n",
    "    tempMax[\"Week Number\"] = pandas.DataFrame (get_week_of_month(int(j.split(\"-\")[0]),\n",
    "                            int(j.split(\"-\")[1]),\n",
    "                            int(j.split(\"-\")[2])\n",
    "                            )/5 for j in tempMax[\"Date\"])\n",
    "    \n",
    "    #Select safe max for each entry\n",
    "    priceMinMax = History[\"Price Min: Safe Max\"][History[\"Entry ID\"] == _]\n",
    "    priceMaxMax = History[\"Price Max: Safe Max\"][History[\"Entry ID\"] == _]\n",
    "    \n",
    "    #Apply scaling over price and covert to series to supervised as timeseries object\n",
    "    tempMin[[\"T-3\", \"T-2\", \"T-1\", \"T\"]] = series_to_supervised(minmax_scale(pandas.DataFrame(dataMin[_]), 0, priceMinMax.iloc[0]), 3, 1)\n",
    "    tempMax[[\"T-3\", \"T-2\", \"T-1\", \"T\"]] = series_to_supervised(minmax_scale(pandas.DataFrame(dataMax[_]), 0, priceMaxMax.iloc[0]), 3, 1)\n",
    "\n",
    "    \n",
    "    #Delete all missing values\n",
    "    tempMin = tempMin[tempMin['T-3'].notna()]\n",
    "    tempMin = tempMin[tempMin['T-2'].notna()]\n",
    "    tempMin = tempMin[tempMin['T-1'].notna()]\n",
    "    tempMin = tempMin[tempMin['T'].notna()]\n",
    "    \n",
    "    tempMax = tempMax[tempMax['T-3'].notna()]\n",
    "    tempMax = tempMax[tempMax['T-2'].notna()]\n",
    "    tempMax = tempMax[tempMax['T-1'].notna()]\n",
    "    tempMax = tempMax[tempMax['T'].notna()]\n",
    "\n",
    "    #Shuffle the data and reset it indexes\n",
    "    tempMin = tempMin.sample(n=len(tempMin), random_state=9).reset_index(drop=True)\n",
    "    tempMax = tempMax.sample(n=len(tempMax), random_state=9).reset_index(drop=True)\n",
    "    \n",
    "    # Change to percentage or set threshold to 0, in order to consider all entries\n",
    "    if len(tempMin) >= 10 and len(tempMax) >= 10:\n",
    "        \n",
    "        #Change Trainable instance to true\n",
    "        History[\"Trainable\"][History[\"Entry ID\"] == _] = True\n",
    "        \n",
    "        #Extract features from entry data\n",
    "        XMin = tempMin[tempMin.columns.tolist()[:-1]]\n",
    "        XMax = tempMax[tempMax.columns.tolist()[:-1]]\n",
    "        \n",
    "        #Extract labels from entry data\n",
    "        YMin = tempMin[tempMin.columns.tolist()[-1:]]\n",
    "        YMax = tempMax[tempMax.columns.tolist()[-1:]]\n",
    "\n",
    "        #Split data into 70% Training and 30% Testing, while storing them to disk \n",
    "        XMin[:int(len(XMin) * TrainSplit)].to_csv(PriceMinTrainPathX + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Min: Train X\"][History[\"Entry ID\"] == _] = PriceMinTrainPathX + \"/\" + str(int(_)) + \".csv\"\n",
    "        YMin[:int(len(YMin) * TrainSplit)].to_csv(PriceMinTrainPathY + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Min: Train Y\"][History[\"Entry ID\"] == _] = PriceMinTrainPathY + \"/\" + str(int(_)) + \".csv\"\n",
    "        XMin[int(len(XMin) * TrainSplit):].to_csv(PriceMinTestPathX + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Min: Test X\"][History[\"Entry ID\"] == _] = PriceMinTestPathX + \"/\" + str(int(_)) + \".csv\"\n",
    "        YMin[int(len(YMin) * TrainSplit):].to_csv(PriceMinTestPathY + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Min: Test Y\"][History[\"Entry ID\"] == _] = PriceMinTestPathY + \"/\" + str(int(_)) + \".csv\"\n",
    "        \n",
    "        XMax[:int(len(XMax) * TrainSplit)].to_csv(PriceMaxTrainPathX + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Max: Train X\"][History[\"Entry ID\"] == _] = PriceMaxTrainPathX + \"/\" + str(int(_)) + \".csv\"\n",
    "        YMax[:int(len(YMax) * TrainSplit)].to_csv(PriceMaxTrainPathY + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Max: Train Y\"][History[\"Entry ID\"] == _] = PriceMaxTrainPathY + \"/\" + str(int(_)) + \".csv\"\n",
    "        XMax[int(len(XMax) * TrainSplit):].to_csv(PriceMaxTestPathX + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Max: Test X\"][History[\"Entry ID\"] == _] = PriceMaxTestPathX + \"/\" + str(int(_)) + \".csv\"\n",
    "        YMax[int(len(YMax) * TrainSplit):].to_csv(PriceMaxTestPathY + \"/\" + str(int(_)) + \".csv\",index=False)\n",
    "        History[\"Price Max: Test Y\"][History[\"Entry ID\"] == _] = PriceMaxTestPathY + \"/\" + str(int(_)) + \".csv\"\n",
    "        \n",
    "    tempMin = None\n",
    "    tempMax = None\n",
    "\n",
    "dataMin = None\n",
    "dataMax = None\n",
    "dataAvg = None\n",
    "\n",
    "History.to_csv(\"History.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = pandas.DataFrame(History[\"Entry ID\"]).rename(columns={0:\"Entry ID\"})\n",
    "for _ in modelList:\n",
    "    Model[\"Price Min: \" + _] = None\n",
    "    Model[\"Price Min: \" + _ + \": Train Loss\"] = None\n",
    "    Model[\"Price Min: \" + _ + \": Test Loss\"] = None\n",
    "    Model[\"Price Min: \" + _ + \": Train Time\"] = None\n",
    "    Model[\"Price Max: \" + _] = None\n",
    "    Model[\"Price Max: \" + _ + \": Train Loss\"] = None\n",
    "    Model[\"Price Max: \" + _ + \": Test Loss\"] = None\n",
    "    Model[\"Price Max: \" + _ + \": Train Time\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ecfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Loop over considered models\n",
    "\"\"\"\n",
    "for _ in modelList:\n",
    "    print(\"Training Model: \" + _ )\n",
    "    pathMin = \"Model/\" + _ + \"/Price Min\"\n",
    "    pathMax = \"Model/\" + _ + \"/Price Max\"\n",
    "    makeDir(pathMin)\n",
    "    makeDir(pathMax)\n",
    "    \n",
    "    for i in History[\"Entry ID\"][History[\"Trainable\"] == True].astype(int).tolist():\n",
    "        \n",
    "        trainX = pandas.read_csv(History[\"Price Min: Train X\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        trainY = pandas.read_csv(History[\"Price Min: Train Y\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        testX = pandas.read_csv(History[\"Price Min: Test X\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        testY = pandas.read_csv(History[\"Price Min: Test Y\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        start_time = time.time()\n",
    "        trainLoss, testLoss, weigths = locals()[modelList[_]](trainX[:,1:], trainY, testX[:,1:], testY)\n",
    "        path = pathMin + \"/\" + str(i) + \".pkl\"\n",
    "        dumpModel(path, weigths)\n",
    "        end_time = time.time() - start_time\n",
    "        Model[\"Price Min: \" + _][Model[\"Entry ID\"] == i] = path\n",
    "        Model[\"Price Min: \" + _ + \": Train Loss\"][Model[\"Entry ID\"] == i] = trainLoss\n",
    "        Model[\"Price Min: \" + _ + \": Test Loss\"][Model[\"Entry ID\"] == i] = testLoss\n",
    "        Model[\"Price Min: \" + _ + \": Train Time\"][Model[\"Entry ID\"] == i] = end_time\n",
    "        \n",
    "        trainX = pandas.read_csv(History[\"Price Max: Train X\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        trainY = pandas.read_csv(History[\"Price Max: Train Y\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        testX = pandas.read_csv(History[\"Price Max: Test X\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        testY = pandas.read_csv(History[\"Price Max: Test Y\"][History[\"Entry ID\"] == i].tolist()[-1]).values\n",
    "        start_time = time.time()\n",
    "        trainLoss, testLoss, weigths = locals()[modelList[_]](trainX[:,1:], trainY, testX[:,1:], testY)\n",
    "        path = pathMax + \"/\" + str(i) + \".pkl\"\n",
    "        dumpModel(path, weigths)\n",
    "        end_time = time.time() - start_time\n",
    "        Model[\"Price Max: \" + _][Model[\"Entry ID\"] == i] = path\n",
    "        Model[\"Price Max: \" + _ + \": Train Loss\"][Model[\"Entry ID\"] == i] = trainLoss\n",
    "        Model[\"Price Max: \" + _ + \": Test Loss\"][Model[\"Entry ID\"] == i] = testLoss\n",
    "        Model[\"Price Max: \" + _ + \": Train Time\"][Model[\"Entry ID\"] == i] = end_time\n",
    "print(\"Models has been trained succesfully, refer to History\")\n",
    "Model.to_csv(\"Model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78154adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = pandas.read_csv(\"Model.csv\")\n",
    "Results = list()\n",
    "for _ in modelList:\n",
    "    TrainMin = Model[\"Price Min: \" + _ + \": Train Loss\"].mean()\n",
    "    TestMin = Model[\"Price Min: \" + _ + \": Test Loss\"].mean()\n",
    "    TimeMin = Model[\"Price Min: \" + _ + \": Train Time\"].sum()\n",
    "    TrainMax = Model[\"Price Max: \" + _ + \": Train Loss\"].mean()\n",
    "    TestMax = Model[\"Price Max: \" + _ + \": Test Loss\"].mean()\n",
    "    TimeMax = Model[\"Price Max: \" + _ + \": Train Time\"].sum()\n",
    "    Results.append([_, TrainMin, TestMin, TimeMin, TrainMax, TestMax, TimeMax])\n",
    "\n",
    "Results = pandas.DataFrame(Results).rename(columns = {0:\"Model\",\n",
    "                                                      1:\"Price Min: Train Loss\",\n",
    "                                                      2:\"Price Min: Test Loss\",\n",
    "                                                      3:\"Price Min: Train Time\",\n",
    "                                                      4:\"Price Max: Train Loss\",\n",
    "                                                      5:\"Price Max: Test Loss\",\n",
    "                                                      6:\"Price Max: Train Time\"})\n",
    "\n",
    "for _ in Results.columns[Results.columns.str.contains(\"Loss\")].tolist():\n",
    "    Results[_][Results[_] > 1] = 1\n",
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(Results,\n",
    "             x=Results[\"Model\"],\n",
    "             y=Results[\"Price Min: Train Loss\"],\n",
    "             labels=dict(x=\"Model\", y=\"Loss\", color=\"Labels\"),\n",
    "             color=px.Constant(\"Train Loss\"),\n",
    "             barmode='group',\n",
    "             title = \"Price Min: Train Loss vs Test Loss\"\n",
    "            )\n",
    "fig.add_bar(x=Results[\"Model\"],\n",
    "            y=Results[\"Price Min: Test Loss\"],\n",
    "            name=\"Test Loss\"\n",
    "            )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513afeaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.bar(Results,\n",
    "             x=Results[\"Model\"],\n",
    "             y=Results[\"Price Max: Train Loss\"],\n",
    "             labels=dict(x=\"Date\", y=\"Loss\", color=\"Labels\"),\n",
    "             color=px.Constant(\"Train Loss\"),\n",
    "             barmode='group',\n",
    "             title = \"Price Max: Train Loss vs Test Loss\"\n",
    "            )\n",
    "fig.add_bar(x=Results[\"Model\"],\n",
    "            y=Results[\"Price Max: Test Loss\"],\n",
    "            name=\"Test Loss\"\n",
    "            )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29874997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(Results,\n",
    "             x=Results[\"Model\"],\n",
    "             y=Results[\"Price Max: Train Time\"],\n",
    "             labels=dict(x=\"Model\", y=\"Time\", color=\"Labels\"),\n",
    "             barmode='group',\n",
    "             title = \"Training Time\"\n",
    "            )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545129fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results.to_csv(\"Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05008b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempStorage = list()\n",
    "path = \"Analysis\"\n",
    "makeDir(path)\n",
    "for _ in History[\"Entry ID\"][History[\"Trainable\"] == True]:\n",
    "    Min = 0\n",
    "    PriceMin_Max = History[\"Price Min: Safe Max\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    PriceMax_Max = History[\"Price Max: Safe Max\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    \n",
    "    DataMin = pandas.read_csv(History[\"Price Min: Train X\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    DataMax = pandas.read_csv(History[\"Price Max: Train X\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    \n",
    "    DataMin[\"T\"] = pandas.read_csv(History[\"Price Min: Train Y\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    temp = pandas.read_csv(History[\"Price Min: Test X\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    temp[\"T\"] = pandas.read_csv(History[\"Price Min: Test Y\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    DataMin = pandas.concat([DataMin, temp])\n",
    "    DataMin = DataMin[[\"Date\", \"T\"]]\n",
    "    temp = None\n",
    "    \n",
    " \n",
    "    DataMax[\"T\"] = pandas.read_csv(History[\"Price Max: Train Y\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    temp = pandas.read_csv(History[\"Price Max: Test X\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    temp[\"T\"] = pandas.read_csv(History[\"Price Max: Test Y\"][History[\"Entry ID\"] == _].iloc[0])\n",
    "    DataMax = pandas.concat([DataMax, temp])\n",
    "    DataMax = DataMax[[\"Date\", \"T\"]]\n",
    "    temp = None\n",
    "    \n",
    "    DataMin = DataMin.sort_values([\"Date\"],ascending=True).reset_index(drop=True)\n",
    "    DataMax = DataMax.sort_values([\"Date\"],ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    total = len(DataMin)\n",
    "    Start = DataMin[\"Date\"].iloc[0]\n",
    "    Start = date(int(Start.split(\"-\")[0]),\n",
    "                 int(Start.split(\"-\")[1]),\n",
    "                 int(Start.split(\"-\")[2]))\n",
    "    End = date.today()\n",
    "    Frequency = \"w\"\n",
    "    \n",
    "    DataMin = collectMissingData(DataMin, Start, End, Frequency)\n",
    "    DataMax = collectMissingData(DataMax, Start, End, Frequency)\n",
    "    \n",
    "    DataMin[\"Predicted\"] = numpy.nan\n",
    "    DataMax[\"Predicted\"] = numpy.nan\n",
    "    \n",
    "    LoadModelMin = pickle.load(open(Model[\"Price Min: Neural Network\"][Model[\"Entry ID\"] == _].iloc[0], 'rb'))\n",
    "    LoadModelMax = pickle.load(open(Model[\"Price Max: Neural Network\"][Model[\"Entry ID\"] == _].iloc[0], 'rb'))\n",
    "    \n",
    "    MissingDataIndex = DataMin[DataMin[\"T\"].isnull()].index.tolist()\n",
    "    if len(MissingDataIndex) > 0:\n",
    "        for i in MissingDataIndex:\n",
    "            Date = DataMin[\"Date\"][DataMin.index == i].iloc[0]\n",
    "            MonthNumber = round(int(Date.split(\"-\")[1])/12, 2)\n",
    "            WeekNumber = get_week_of_month(int(Date.split(\"-\")[0]),\n",
    "                                           int(Date.split(\"-\")[1]),\n",
    "                                           int(Date.split(\"-\")[2]))/5\n",
    "            WeekNumber = WeekNumber[0]\n",
    "            try: T_3 = DataMax[\"T\"][DataMax.index == i - 3].fillna(0).iloc[0]\n",
    "            except: T_3 = 0\n",
    "            try: T_2 = DataMax[\"T\"][DataMax.index == i - 2].fillna(0).iloc[0]\n",
    "            except: T_2 = 0\n",
    "            try: T_1 = DataMax[\"T\"][DataMax.index == i - 1].fillna(0).iloc[0]\n",
    "            except: T_1 = 0\n",
    "            try:\n",
    "                T = abs(LoadModelMin.predict(pandas.DataFrame([MonthNumber, WeekNumber, T_3, T_2, T_1]).T)[0])\n",
    "                DataMin[\"T\"][DataMin.index == i] = T\n",
    "                DataMin[\"Predicted\"][DataMin.index == i] = T\n",
    "            except:\n",
    "                T = 0\n",
    "                DataMin[\"T\"][DataMin.index == i] = T\n",
    "                DataMin[\"Predicted\"][DataMin.index == i] = T\n",
    "                \n",
    "    DataMin[\"T\"] = DataMin[\"T\"] * PriceMin_Max\n",
    "    DataMin['Predicted'] = DataMin['Predicted'] * PriceMin_Max\n",
    "    \n",
    "    \n",
    "    MissingDataIndex = DataMax[DataMax[\"T\"].isnull()].index.tolist()\n",
    "    if len(MissingDataIndex) > 0:\n",
    "        for i in MissingDataIndex:\n",
    "            Date = DataMax[\"Date\"][DataMax.index == i].iloc[0]\n",
    "            MonthNumber = round(int(Date.split(\"-\")[1])/12, 2)\n",
    "            WeekNumber = get_week_of_month(int(Date.split(\"-\")[0]),\n",
    "                                           int(Date.split(\"-\")[1]),\n",
    "                                           int(Date.split(\"-\")[2]))/5\n",
    "            WeekNumber = WeekNumber[0]\n",
    "            try: T_3 = DataMax[\"T\"][DataMax.index == i - 3].fillna(0).iloc[0]\n",
    "            except: T_3 = 0\n",
    "            try: T_2 = DataMax[\"T\"][DataMax.index == i - 2].fillna(0).iloc[0]\n",
    "            except: T_2 = 0\n",
    "            try: T_1 = DataMax[\"T\"][DataMax.index == i - 1].fillna(0).iloc[0]\n",
    "            except: T_1 = 0\n",
    "            try:\n",
    "                T = abs(LoadModelMax.predict(pandas.DataFrame([MonthNumber, WeekNumber, T_3, T_2, T_1]).T)[0])\n",
    "                DataMax[\"T\"][DataMin.index == i] = T\n",
    "                DataMax[\"Predicted\"][DataMin.index == i] = T\n",
    "            except:\n",
    "                T = 0\n",
    "                DataMax[\"T\"][DataMin.index == i] = T\n",
    "                DataMax[\"Predicted\"][DataMin.index == i] = T\n",
    "    DataMax[\"T\"] = DataMax[\"T\"] * PriceMax_Max\n",
    "    DataMax['Predicted'] = DataMax['Predicted'] * PriceMax_Max\n",
    "    \n",
    "    percentage = History[\"Missing Percentage From Max Count\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    grade = History[\"Grade\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "                                                            \n",
    "    fig = px.line(\n",
    "                 x=DataMin['Date'],\n",
    "                 y=(DataMin['T'] + DataMax['T'])/2,\n",
    "                 labels=dict(x=\"Date\", y=\"Price\", color=\"Labels\"),\n",
    "                 color=px.Constant(\"Price Average\"),\n",
    "                 title = str(_) +\"/Missing Percentage-\" + \"{:.2f}\".format(percentage) + \"%\" + \"/Grade-\" + grade\n",
    "                    )\n",
    "    fig['data'][0]['line']['color']='rgba(255, 0, 0, 0.4)'\n",
    "    fig.add_scatter(x=DataMin['Date'],\n",
    "                   y=DataMin['T'],\n",
    "                   mode = \"lines\",\n",
    "                   fill='tozeroy',\n",
    "                   fillcolor= \"rgba(255, 0, 0, 0.1)\",\n",
    "                   line={\"color\":\"rgba(255, 0, 0, 0)\"},\n",
    "                   name=\"Price Min\"\n",
    "                   )\n",
    "    fig.add_scatter(x=DataMax['Date'],\n",
    "                    y=DataMax['T'],\n",
    "                    mode = \"lines\",\n",
    "                    fill='tozeroy',\n",
    "                    fillcolor= \"rgba(255, 0, 0, 0.1)\",\n",
    "                    line={\"color\":\"rgba(255, 0, 0, 0)\"},\n",
    "                    name=\"Price Max\"\n",
    "               )\n",
    "    fig.add_scatter(x=DataMax['Date'],\n",
    "                    y=(DataMin['Predicted'] + DataMax['Predicted'])/2,\n",
    "                    mode = \"lines\",\n",
    "                    line={\"color\":\"rgba(255, 0, 0, 1)\"},\n",
    "                    name=\"Price Average - Imputed\",\n",
    "                    marker = {'color':'red'}\n",
    "                   )\n",
    "    fig.add_scatter(x=DataMax['Date'],\n",
    "                    y=DataMax['Predicted'].fillna(0),\n",
    "                    mode = \"lines\",\n",
    "                    fill='tozeroy',\n",
    "                    fillcolor= \"rgba(255, 0, 0, 0.3)\",\n",
    "                    line={\"color\":\"rgba(255, 0, 0, 0)\"},\n",
    "                    name=\"Imputation Highlight\"\n",
    "               )\n",
    "    #fig.show()\n",
    "    fig.write_html(path + \"/\" + str(_) +\"-\"+grade+\".html\")\n",
    "\n",
    "    temp = DataMin.rename(columns={\"Date\":\"date\",\"T\":\"price_min\", \"Predicted\":\"is_estimated\"})\n",
    "    temp[\"price_max\"] = DataMax[\"T\"]\n",
    "    temp[\"is_estimated\"] = temp[\"is_estimated\"].fillna(False)\n",
    "    temp[\"is_estimated\"][temp[\"is_estimated\"] != False] = True\n",
    "    temp[\"price_avg\"] = (temp[\"price_min\"] + temp[\"price_max\"])/2\n",
    "    temp[\"entry_id\"] = _\n",
    "    temp[\"item_id\"] = History[\"Item ID\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    temp[\"item_unit_id\"] = History[\"Item Unit ID\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    temp[\"period\"] = History[\"Period\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    temp[\"currency\"] = History[\"Currency\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    error = (Model[\"Price Min: Neural Network: Test Loss\"][Model[\"Entry ID\"] == _].iloc[0] + Model[\"Price Max: Neural Network: Test Loss\"][Model[\"Entry ID\"] == _].iloc[0])/2\n",
    "    temp[\"error_rate\"] = error\n",
    "    temp[\"change\"] = ((temp[\"price_avg\"] - temp[\"price_avg\"].shift(-1))/temp[\"price_avg\"].shift(-1)).fillna(0)\n",
    "    temp[\"grade\"] = History[\"Grade\"][History[\"Entry ID\"] == _].iloc[0]\n",
    "    if error <= 0.30:\n",
    "        temp[\"error_level\"] = \"Low\"\n",
    "    elif error > 0.30 and error <= 0.60:\n",
    "        temp[\"error_level\"] = \"Medium\"\n",
    "    else:\n",
    "        temp[\"error_level\"] = \"High\"\n",
    "    temp = temp[[\"date\", \"entry_id\", \"item_id\", \"item_unit_id\", \"price_min\", \"price_max\", \"price_avg\",\n",
    "                 \"period\", \"change\", \"currency\", \"is_estimated\", \"error_rate\", \"error_level\", \"grade\"]]\n",
    "    tempStorage.append(temp)\n",
    "    temp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tempStorage[0]\n",
    "for _ in tempStorage[1:]:\n",
    "    data = pandas.concat([data, _])\n",
    "data = data.reset_index(drop=True)\n",
    "data.to_csv(\"Output.csv\")\n",
    "print(\"Output has been generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e78d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
